{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bace4eb8-1fce-40d1-a070-cd7eae65f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  \n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import math\n",
    "import io\n",
    "import umap.plot\n",
    "import plotly.graph_objs as go \n",
    "import plotly.io as pio \n",
    "pio.renderers.default ='iframe'\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import random\n",
    "from PIL import Image  # Add this import statement\n",
    "\n",
    "import math,os,sys\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a91ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_cnn_dataset import TransformedDataset  \n",
    "#load dataset \n",
    "data = pd.read_csv('../data/train.csv')\n",
    "#data_shorted = data[:10000]\n",
    "#data_clp = data[:10000].reset_index(drop=True) # the index has to be readjusted otherwise it follows the whole data index... problematic\n",
    "#temporarly trying to overfit with less data\n",
    "\n",
    "\n",
    "#common transformation\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01bb093e-188a-4cdf-a199-d5b5237e0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = TransformedDataset(data_clp, default_transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(transformed_data))\n",
    "val_size = len(transformed_data) - train_size\n",
    "train_dataset, val_dataset = random_split(transformed_data, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e45229-22d3-48cf-a33f-dec46c83d673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de91ca38-4641-43fc-a054-9af4e0e45918",
   "metadata": {},
   "source": [
    "# visualizing Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "431a76d4-b95e-49ca-a943-7d9dd320ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title =''):\n",
    "    num_images = len(images)\n",
    "    fig,axes = plt.subplots(1, num_images,figsize=(9,3))\n",
    "    for i in range(num_images):\n",
    "        img = np.squeeze(images[i])\n",
    "        axes[i].imshow(img,cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68115e5-2935-48f5-a968-12f4fe679b70",
   "metadata": {},
   "source": [
    "# lets build Neural Network\n",
    "-  Define a neural network architecture with two convolution layers and two fully connected layers\n",
    "- Input to the network is an MNIST image and Output is a 64 dimensional representation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f725e1e-0423-47f8-b173-8ab9d05296eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network import Network,Network_t ,ContrastiveLoss_with_margin, CustomVGG\n",
    "from utils import init_weights, init_weights_for_gelu,initialize_weights_mod,plot_activation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a4f49e1-4752-453a-8522-4ee463060cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CustomVGG()\n",
    "\n",
    "# Load the VGG16 model unmodiefied first layer(to work for grey -> channel from 3 ->1) and last layers(change number of classes for the last classifier)\n",
    "#net = models.vgg16(pretrained=False)\n",
    "\n",
    "\n",
    "device= \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device= \"mps\"\n",
    "\n",
    "#device= \"cpu\" #overide device for overfitting a very small data batch\n",
    "#net = net.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "814e9a3b-c4d0-4829-96e7-c677c02cc231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0766,  0.2283, -0.3914,  ...,  0.1766, -0.0928,  0.3589],\n",
       "        [-0.1106,  0.3180, -1.1045,  ...,  0.1010, -0.1303, -0.0363],\n",
       "        [ 0.0388, -0.1575, -0.1942,  ...,  0.1134,  0.9738,  0.0049],\n",
       "        ...,\n",
       "        [ 0.0218,  0.5316,  0.6599,  ..., -0.5645,  0.2449,  0.3607],\n",
       "        [-0.1174, -1.0322, -0.6453,  ..., -0.0215, -0.3553,  0.3723],\n",
       "        [-1.0890, -0.3098,  0.6969,  ..., -0.2885, -0.1753, -0.6239]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the VGG16 model without pretrained weights\n",
    "vgg16 = models.vgg16(pretrained=False)\n",
    "\n",
    "# Modify the first convolutional layer to accept grayscale images\n",
    "vgg16.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Optionally, initialize the weights of the modified layer\n",
    "nn.init.kaiming_normal_(vgg16.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the last fully connected layer to have the desired number of output features\n",
    "# For example, if you have 10 classes:\n",
    "num_classes = 10\n",
    "vgg16.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "# Optionally, initialize the weights of the modified layer\n",
    "nn.init.kaiming_normal_(vgg16.classifier[6].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "#net = vgg16\n",
    "# Print the modified model architecture to verify the changes\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213cc3d-bbd1-4d3c-8189-9c0581e0334d",
   "metadata": {},
   "source": [
    "### optimzer initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f192860c-848f-422a-b0c6-fdff6d3e8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79f8b27f-d130-4697-a2cf-a1efe0de0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count=10\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr =3e-6,weight_decay=1e-5)\n",
    "#optimizer = torch.optim.AdamW(net.parameters())\n",
    "loss_function = ContrastiveLoss_with_margin()\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epoch_count, eta_min=1e-6)\n",
    "#scheduler reduces plateau loss\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1074f81-4ec1-4d6f-90db-51f9425197f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir ='checkpoints/'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ebe3c-5e66-4359-9069-7bee710666e9",
   "metadata": {},
   "source": [
    "## Testing the state of the model defination of it works for simple digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395d2e3-16ef-4795-bde5-33fabfb7bd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de31c6ca-1b52-4b19-854b-117ec577514e",
   "metadata": {},
   "source": [
    "## experimaental model with simple image classification to check model health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a135e-d587-4134-a096-aa0218b4da52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mtm007/Downloads/multiModal/Multi-modal-Training-/src/wandb/run-20240708_152714-0jswlpp9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajax_m/Contrastive_learning/runs/0jswlpp9/workspace' target=\"_blank\">devout-bee-42</a></strong> to <a href='https://wandb.ai/ajax_m/Contrastive_learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajax_m/Contrastive_learning' target=\"_blank\">https://wandb.ai/ajax_m/Contrastive_learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajax_m/Contrastive_learning/runs/0jswlpp9/workspace' target=\"_blank\">https://wandb.ai/ajax_m/Contrastive_learning/runs/0jswlpp9/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 3e-06\n",
      "Train Epoch: 0 [0/8000 (0%)]\tLoss: 0.360429\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Contrastive_learning\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"architecture\": \"vgg16 experimental model health\",\n",
    "    \"dataset\": \"Mnist -dataset\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch size\" : 128,\n",
    "    }\n",
    ")\n",
    "\n",
    "epochs =10\n",
    "activations_list = []\n",
    "gradients = []\n",
    "\n",
    "def training_model(epoch=epochs):\n",
    "    net = CustomVGG()\n",
    "    #The log=\"all\" parameter tells wandb to log gradients and parameters, and \n",
    "    #log_freq=64 means it will log every 64 batches.\n",
    "    wandb.watch(net, log=\"all\", log_freq=64) \n",
    "    #net.apply(init_weights_for_gelu)\n",
    "    net = net.to(device)\n",
    "    initialize_weights_mod(net, activation_function='leaky_relu')\n",
    "    \n",
    "    lrs = []\n",
    "    losses = []\n",
    "    activations_dict = defaultdict(lambda: {'mean': [], 'var': [], 'neg_ratio': []})\n",
    "    \n",
    "    def get_activation_stats(name):\n",
    "        def hook(model, input, output):\n",
    "            mean = output.detach().mean().item()\n",
    "            var = output.detach().var().item()\n",
    "            neg_ratio = (output.detach() < 0).float().mean().item()\n",
    "            activations_dict[name]['mean'].append(mean)\n",
    "            activations_dict[name]['var'].append(var)\n",
    "            activations_dict[name]['neg_ratio'].append(neg_ratio)\n",
    "            # this was not printing coz of leakyReLU wasnt correctly called(was only nn.ReLU)\n",
    "            #print(f'Hook called for {name}: mean={mean}, var={var}, neg_ratio={neg_ratio}')\n",
    "        return hook   \n",
    "    \n",
    "    # Register hooks for GELU/ReLU layers depends (or whatever activation you're using)\n",
    "    for name, layer in net.named_modules():\n",
    "        if isinstance(layer, nn.LeakyReLU):\n",
    "            layer.register_forward_hook(get_activation_stats(name))\n",
    "            #print(f'Registered hook for layer: {name}') #-----debugging print worked\n",
    "            \n",
    "    def capture_gradient(name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            gradients.append((name, grad_output[0].detach()))\n",
    "        return hook\n",
    "    \n",
    "    #model.train()\n",
    "    for epoch in range(epoch):\n",
    "        epoch_loss = 0\n",
    "        batches = 0\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lrs.append(param_group['lr'])\n",
    "        \n",
    "        print('learning rate', lrs[-1])\n",
    "        wandb.log({'learning rate': lrs[-1]})\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            epoch_loss += loss.item()\n",
    "            batches += 1\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "            \n",
    "                #Compute values  for wandb logging\n",
    "                current_step = batch_idx * len(data)\n",
    "                total_steps = len(train_loader.dataset)\n",
    "                percentage_complete = 100. * batch_idx / len(train_loader)\n",
    "                loss_value = loss.item()\n",
    "        \n",
    "                # Log the values with wandb\n",
    "                wandb.log({\n",
    "                    'epoch': epoch,\n",
    "                    'current_step': current_step,\n",
    "                    'total_steps': total_steps,\n",
    "                    'percentage_complete': percentage_complete,\n",
    "                    'loss': loss_value\n",
    "                })\n",
    "                \n",
    "            # Average the epoch loss over batches\n",
    "            #fixing the different length of lr and losses items (10 vs 630) by averaging loss per epoch\n",
    "        losses.append(epoch_loss / batches)\n",
    "    \n",
    "        activations_list.append({k: {stat: v[stat][-batches:] for stat in v} for k, v in activations_dict.items()})\n",
    "        #losses.append(loss.cpu().detach().numpy() / batches)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #epoch_loss /= len(train_loader)\n",
    "        #print(f\"Epoch {epoch}: Average Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "    plot_activation_stats(activations_list)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(lrs, losses)\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Rate vs. Loss')\n",
    "    plt.show()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert BytesIO to PIL Image\n",
    "    image1 = Image.open(buf)\n",
    "    wandb.log({\"Learning Rate vs. Loss\": wandb.Image(image1)})\n",
    "\n",
    "    return {\n",
    "        \"net\": net,\n",
    "        \"losses\": losses,\n",
    "        \"activations\": activations_list\n",
    "    }\n",
    "\n",
    "\n",
    "training_result = training_model()\n",
    "model = training_result[\"net\"]\n",
    "\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd84c09d-756b-40ad-b520-c29a4a3282e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            val_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f'\\nValidation set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{len(val_loader.dataset)} ({100. * correct / len(val_loader.dataset):.0f}%)\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2a8f8-b840-49ec-9080-34e1256375db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network().to(device)\n",
    "\n",
    "#torch compile\n",
    "m = torch.compile(model)\n",
    "\n",
    "#wandb watch\n",
    "wandb.watch(m)\n",
    "\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 00.0001)\n",
    "lossi = []\n",
    "for iter in range(max_iters): \n",
    "    if iter % eval_iterval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        #wandb log\n",
    "        wandb.log({\"steps\": iter,\"train_loss\": losses[\"train\"], \"val_loss\": losses[\"val\"]})\n",
    "\n",
    "    xb,yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    lr =  lr if max_iters > 20000 else lr*10    #step learing rate Decay \n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

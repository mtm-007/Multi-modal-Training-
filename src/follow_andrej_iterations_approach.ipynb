{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bace4eb8-1fce-40d1-a070-cd7eae65f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  \n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import math\n",
    "import io\n",
    "import umap.plot\n",
    "import plotly.graph_objs as go \n",
    "import plotly.io as pio \n",
    "pio.renderers.default ='iframe'\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import random\n",
    "from PIL import Image  # Add this import statement\n",
    "\n",
    "%matplotlib inline\n",
    "import math,os,sys\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8a91ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_cnn_dataset import TransformedDataset  \n",
    "#load dataset \n",
    "data = pd.read_csv('../data/train.csv')\n",
    "#data_shorted = data[:10000]\n",
    "data_clp = data[:10000].reset_index(drop=True) # the index has to be readjusted otherwise it follows the whole data index... problematic\n",
    "#temporarly trying to overfit with less data\n",
    "\n",
    "\n",
    "#common transformation\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01bb093e-188a-4cdf-a199-d5b5237e0af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = TransformedDataset(data_clp, default_transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(transformed_data))\n",
    "val_size = len(transformed_data) - train_size\n",
    "train_dataset, val_dataset = random_split(transformed_data, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e45229-22d3-48cf-a33f-dec46c83d673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de91ca38-4641-43fc-a054-9af4e0e45918",
   "metadata": {},
   "source": [
    "# visualizing Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "431a76d4-b95e-49ca-a943-7d9dd320ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title =''):\n",
    "    num_images = len(images)\n",
    "    fig,axes = plt.subplots(1, num_images,figsize=(9,3))\n",
    "    for i in range(num_images):\n",
    "        img = np.squeeze(images[i])\n",
    "        axes[i].imshow(img,cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68115e5-2935-48f5-a968-12f4fe679b70",
   "metadata": {},
   "source": [
    "# lets build Neural Network\n",
    "-  Define a neural network architecture with two convolution layers and two fully connected layers\n",
    "- Input to the network is an MNIST image and Output is a 64 dimensional representation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f725e1e-0423-47f8-b173-8ab9d05296eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network import Network,Network_t ,ContrastiveLoss_with_margin, CustomVGG\n",
    "from utils import init_weights, init_weights_for_gelu,initialize_weights_mod,plot_activation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a4f49e1-4752-453a-8522-4ee463060cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CustomVGG()\n",
    "\n",
    "# Load the VGG16 model unmodiefied first layer(to work for grey -> channel from 3 ->1) and last layers(change number of classes for the last classifier)\n",
    "#net = models.vgg16(pretrained=False)\n",
    "\n",
    "\n",
    "device= \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device= \"mps\"\n",
    "\n",
    "#device= \"cpu\" #overide device for overfitting a very small data batch\n",
    "#net = net.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "814e9a3b-c4d0-4829-96e7-c677c02cc231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4331,  0.3713, -0.5005,  ...,  0.6261, -0.2311,  0.0837],\n",
       "        [-0.3836,  0.1280,  0.2263,  ..., -0.2390, -0.3197,  0.6447],\n",
       "        [-0.6263,  0.1920, -0.1210,  ...,  0.4876, -0.2372, -0.1031],\n",
       "        ...,\n",
       "        [ 0.1582,  0.0473, -0.5436,  ..., -0.4659, -0.4897, -0.1450],\n",
       "        [-0.1888,  0.2101, -0.0926,  ..., -0.2610, -0.6090,  0.4436],\n",
       "        [-0.8256, -0.2650, -0.2683,  ...,  0.6475, -0.3692,  0.4248]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the VGG16 model without pretrained weights\n",
    "vgg16 = models.vgg16(pretrained=False)\n",
    "\n",
    "# Modify the first convolutional layer to accept grayscale images\n",
    "vgg16.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Optionally, initialize the weights of the modified layer\n",
    "nn.init.kaiming_normal_(vgg16.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Modify the last fully connected layer to have the desired number of output features\n",
    "# For example, if you have 10 classes:\n",
    "num_classes = 10\n",
    "vgg16.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "# Optionally, initialize the weights of the modified layer\n",
    "nn.init.kaiming_normal_(vgg16.classifier[6].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "#net = vgg16\n",
    "# Print the modified model architecture to verify the changes\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213cc3d-bbd1-4d3c-8189-9c0581e0334d",
   "metadata": {},
   "source": [
    "### optimzer initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f192860c-848f-422a-b0c6-fdff6d3e8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79f8b27f-d130-4697-a2cf-a1efe0de0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count=10\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr =1e-3,weight_decay=1e-5)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.3)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=epoch_count, eta_min=1e-6)\n",
    "\n",
    "#scheduler reduces plateau loss\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1074f81-4ec1-4d6f-90db-51f9425197f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir ='checkpoints/'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ebe3c-5e66-4359-9069-7bee710666e9",
   "metadata": {},
   "source": [
    "## Testing the state of the model defination of it works for simple digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a24dd8b2-7a0b-42a8-b046-eef45a7a93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3,0,1000)\n",
    "lrse = 10**lre\n",
    "\n",
    "lri=[]\n",
    "lossi =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95a541-c63e-4d08-a474-cd047658e21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e395d2e3-16ef-4795-bde5-33fabfb7bd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:mbg9g1ce) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 18.7%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iteration</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>learning rate</td><td>▁▁▁▁▁▁▂▃▄█</td></tr><tr><td>loss</td><td>▆█▃▄▅▄▇▄▁▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Plotting data for layer</td><td>features.1</td></tr><tr><td>iteration</td><td>1000</td></tr><tr><td>learning rate</td><td>1.0</td></tr><tr><td>loss</td><td>0.46296</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-donkey-76</strong> at: <a href='https://wandb.ai/ajax_m/Contrastive_learning/runs/mbg9g1ce/workspace' target=\"_blank\">https://wandb.ai/ajax_m/Contrastive_learning/runs/mbg9g1ce/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240708_184507-mbg9g1ce/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:mbg9g1ce). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bbfa2bd22c440389990ea924d97ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01117111064441916, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mtm007/Downloads/multiModal/Multi-modal-Training-/src/wandb/run-20240708_190644-ql06slly</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ajax_m/Contrastive_learning/runs/ql06slly/workspace' target=\"_blank\">kind-eon-77</a></strong> to <a href='https://wandb.ai/ajax_m/Contrastive_learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ajax_m/Contrastive_learning' target=\"_blank\">https://wandb.ai/ajax_m/Contrastive_learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ajax_m/Contrastive_learning/runs/ql06slly/workspace' target=\"_blank\">https://wandb.ai/ajax_m/Contrastive_learning/runs/ql06slly/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, Loss: 0.18143178134955265, Learning Rate: 0.001982883783057332\n",
      "Iteration 200, Loss: 0.1683971770107746, Learning Rate: 0.003959110472351313\n",
      "Iteration 300, Loss: 0.1318515344367673, Learning Rate: 0.007904929108917713\n",
      "Iteration 400, Loss: 0.15810897760093212, Learning Rate: 0.015783313661813736\n",
      "Iteration 500, Loss: 0.164185641112469, Learning Rate: 0.03151363879442215\n",
      "Iteration 600, Loss: 0.14482127931533437, Learning Rate: 0.06292146444320679\n",
      "Iteration 700, Loss: 0.12915182645831788, Learning Rate: 0.1256316602230072\n",
      "Iteration 800, Loss: 0.17039820670404218, Learning Rate: 0.25084149837493896\n",
      "Iteration 900, Loss: 0.19577626883983612, Learning Rate: 0.5008407831192017\n",
      "Iteration 1000, Loss: 0.18847470923580906, Learning Rate: 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 108\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m\"\u001b[39m: net,\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m: losses,\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactivations\u001b[39m\u001b[38;5;124m\"\u001b[39m: activations_dict\n\u001b[1;32m    105\u001b[0m     }\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Run the training model function\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m training_result \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m model \u001b[38;5;241m=\u001b[39m training_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    110\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[34], line 92\u001b[0m, in \u001b[0;36mtraining_model\u001b[0;34m(iteration_count)\u001b[0m\n\u001b[1;32m     89\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_iter\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(net\u001b[38;5;241m.\u001b[39mstate_dict(), checkpoint_path)\n\u001b[0;32m---> 92\u001b[0m \u001b[43mplot_activation_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Plot learning rate vs. loss\u001b[39;00m\n\u001b[1;32m     95\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "File \u001b[0;32m~/Downloads/multiModal/Multi-modal-Training-/src/utils.py:79\u001b[0m, in \u001b[0;36mplot_activation_stats\u001b[0;34m(activations_list)\u001b[0m\n\u001b[1;32m     77\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlotting data for layer\u001b[39m\u001b[38;5;124m'\u001b[39m:layer_name}) \u001b[38;5;66;03m# logging this stat only on wandb \u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m#print(f'Means: {means}')\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMeans\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeans\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m#print(f'Variances: {vars}')\u001b[39;00m\n\u001b[1;32m     81\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariances\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mvars\u001b[39m})\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:420\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:371\u001b[0m, in \u001b[0;36m_run_decorator._noop_on_finish.<locals>.decorator_fn.<locals>.wrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m: Type[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_finished\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 371\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     default_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    374\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is finished. The call to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be ignored. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure that you are using an active run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    376\u001b[0m     )\n\u001b[1;32m    377\u001b[0m     resolved_message \u001b[38;5;241m=\u001b[39m message \u001b[38;5;129;01mor\u001b[39;00m default_message\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:361\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1838\u001b[0m, in \u001b[0;36mRun.log\u001b[0;34m(self, data, step, commit, sync)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39m_shared \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1832\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mtermwarn(\n\u001b[1;32m   1833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn shared mode, the use of `wandb.log` with the step argument is not supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1834\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand will be ignored. Please refer to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwburls\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb_define_metric\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1835\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon how to customize your x-axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1836\u001b[0m         repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1837\u001b[0m     )\n\u001b[0;32m-> 1838\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1602\u001b[0m, in \u001b[0;36mRun._log\u001b[0;34m(self, data, step, commit)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey values passed to `wandb.log` must be strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1602\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_partial_history_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetpid() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_pid \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attached:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:1474\u001b[0m, in \u001b[0;36mRun._partial_history_callback\u001b[0;34m(self, row, step, commit)\u001b[0m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface:\n\u001b[1;32m   1472\u001b[0m     not_using_tensorboard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(wandb\u001b[38;5;241m.\u001b[39mpatched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_partial_history\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpublish_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnot_using_tensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:584\u001b[0m, in \u001b[0;36mInterfaceBase.publish_partial_history\u001b[0;34m(self, data, user_step, step, flush, publish_step, run)\u001b[0m\n\u001b[1;32m    582\u001b[0m     item \u001b[38;5;241m=\u001b[39m partial_history\u001b[38;5;241m.\u001b[39mitem\u001b[38;5;241m.\u001b[39madd()\n\u001b[1;32m    583\u001b[0m     item\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m k\n\u001b[0;32m--> 584\u001b[0m     item\u001b[38;5;241m.\u001b[39mvalue_json \u001b[38;5;241m=\u001b[39m \u001b[43mjson_dumps_safer_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m publish_step \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    587\u001b[0m     partial_history\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mnum \u001b[38;5;241m=\u001b[39m step\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/util.py:842\u001b[0m, in \u001b[0;36mjson_dumps_safer_history\u001b[0;34m(obj, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjson_dumps_safer_history\u001b[39m(obj: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    841\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert obj to json, with some extra encodable types, including histograms.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWandBHistoryJSONEncoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/util.py:804\u001b[0m, in \u001b[0;36mWandBHistoryJSONEncoder.default\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    803\u001b[0m     obj, converted \u001b[38;5;241m=\u001b[39m json_friendly(obj)\n\u001b[0;32m--> 804\u001b[0m     obj, compressed \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_compress_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converted:\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/util.py:705\u001b[0m, in \u001b[0;36mmaybe_compress_history\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaybe_compress_history\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m32\u001b[39m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_json(), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/wandb/sdk/data_types/histogram.py:77\u001b[0m, in \u001b[0;36mHistogram.__init__\u001b[0;34m(self, sequence, np_histogram, num_bins)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     np \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_module(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto creation of histograms requires numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     )\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/numpy/lib/histograms.py:793\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m \n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    791\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 793\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/numpy/lib/histograms.py:426\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[43m_get_outer_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/numpy/lib/histograms.py:322\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mmin(), a\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautodetected range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# expand empty range to avoid divide by zero\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Contrastive_learning\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"architecture\": \"vgg16 experimental model health\",\n",
    "    \"dataset\": \"Mnist -dataset\",\n",
    "    \"iteration_count\": 1000,\n",
    "    \"batch size\" : 128,\n",
    "    }\n",
    ")\n",
    "\n",
    "ud=[]\n",
    "def training_model(iteration_count=1000):\n",
    "    net = CustomVGG()\n",
    "    #net.apply(init_weights_for_gelu)\n",
    "    initialize_weights_mod(net, activation_function='relu')\n",
    "    net = net.to(device)\n",
    "    lrs = []\n",
    "    losses = []\n",
    "    activations_dict = defaultdict(lambda: {'mean': [], 'var': [], 'neg_ratio': []})\n",
    "    \n",
    "    def get_activation_stats(name):\n",
    "        def hook(model, input, output):\n",
    "            mean = output.detach().mean().item()\n",
    "            var = output.detach().var().item()\n",
    "            neg_ratio = (output.detach() < 0).float().mean().item()\n",
    "            activations_dict[name]['mean'].append(mean)\n",
    "            activations_dict[name]['var'].append(var)\n",
    "            activations_dict[name]['neg_ratio'].append(neg_ratio)\n",
    "        return hook\n",
    "\n",
    "    for name, layer in net.named_modules():\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            layer.register_forward_hook(get_activation_stats(name))\n",
    "\n",
    "    activations_list = []\n",
    "    gradients = []\n",
    "    ud =[]\n",
    "    iteration = 0\n",
    "    while iteration < iteration_count:\n",
    "        epoch_loss = 0\n",
    "        batches = 0\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            lrs.append(param_group['lr'])\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if iteration >= iteration_count:\n",
    "                break\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            norm = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "            lr = lrse[iteration]\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            #track lr stats\n",
    "            lri.append(lr)\n",
    "            lossi.append(loss.item())\n",
    "            \n",
    "            batches += 1\n",
    "            iteration += 1\n",
    "            \n",
    "            activations_list.append({k: {stat: v[stat][-batches:] for stat in v} for k, v in activations_dict.items()})\n",
    "            losses.append(loss.cpu().detach().numpy() / batches)\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            with torch.no_grad():\n",
    "            #ud.append([(lr*p.grad.std()/ p.data.std().log10().item()) for p in net.parameters()])\n",
    "                epsilon = 1e-8\n",
    "                ud.append([(current_lr * p.grad.std() / (p.data.std() + epsilon).log10().item()) for p in net.parameters() if p.grad is not None])\n",
    "    \n",
    "            \n",
    "            if iteration % 100 == 0:  # Log every 100 iterations\n",
    "                print(f\"Iteration {iteration}, Loss: {epoch_loss / batches}, Learning Rate: {lr}\")\n",
    "                wandb.log({'iteration': iteration, 'loss': epoch_loss / batches, 'learning rate': lr})\n",
    "                \n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_iter{iteration}.pt')\n",
    "        torch.save(net.state_dict(), checkpoint_path)\n",
    "\n",
    "    plot_activation_stats(activations_list)\n",
    "\n",
    "    # Plot learning rate vs. loss\n",
    "    plt.figure()\n",
    "    plt.plot(lri, lossi)\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"net\": net,\n",
    "        \"losses\": losses,\n",
    "        \"activations\": activations_dict\n",
    "    }\n",
    "\n",
    "# Run the training model function\n",
    "training_result = training_model(iteration_count=1000)\n",
    "model = training_result[\"net\"]\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31c6ca-1b52-4b19-854b-117ec577514e",
   "metadata": {},
   "source": [
    "## experimaental model with simple image classification to check model health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174a17a-d55b-45a5-ab89-3b1494f4f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize histogram of gradient\n",
    "plt.figure(figsize=(20,4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(net.parameters()): \n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i].cpu().numpy() for j in range(len(ud))])\n",
    "        legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # those ratios should be ~1e-3, indicated on the plot with black         \n",
    "plt.legend(legends);\n",
    "plt.title('update to data raio distribation, LR setting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d275fc-96e3-418b-9c97-a1af2b7bf5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f4fab-744a-48a6-8f8d-adf27884ed80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbb88b-1aff-485a-832c-38099e3cf63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b4d07-bba6-4c22-aad5-065cf571bf99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea269836-358e-4f2b-bbfe-311e8e6ffd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a135e-d587-4134-a096-aa0218b4da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Contrastive_learning\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"architecture\": \"vgg16 experimental model health\",\n",
    "    \"dataset\": \"Mnist -dataset\",\n",
    "    \"epochs\": 10,\n",
    "    \"batch size\" : 128,\n",
    "    }\n",
    ")\n",
    "\n",
    "epochs =10\n",
    "activations_list = []\n",
    "gradients = []\n",
    "ud =[]\n",
    "\n",
    "def training_model(iteration_count=1000):\n",
    "    net = CustomVGG()\n",
    "    #The log=\"all\" parameter tells wandb to log gradients and parameters, and \n",
    "    #log_freq=64 means it will log every 64 batches.\n",
    "    wandb.watch(net, log=\"all\", log_freq=64) \n",
    "    #net.apply(init_weights_for_gelu)\n",
    "    net = net.to(device)\n",
    "    initialize_weights_mod(net, activation_function='relu')\n",
    "    \n",
    "    lrs = []\n",
    "    losses = []\n",
    "    activations_dict = defaultdict(lambda: {'mean': [], 'var': [], 'neg_ratio': []})\n",
    "    \n",
    "    def get_activation_stats(name):\n",
    "        def hook(model, input, output):\n",
    "            mean = output.detach().mean().item()\n",
    "            var = output.detach().var().item()\n",
    "            neg_ratio = (output.detach() < 0).float().mean().item()\n",
    "            activations_dict[name]['mean'].append(mean)\n",
    "            activations_dict[name]['var'].append(var)\n",
    "            activations_dict[name]['neg_ratio'].append(neg_ratio)\n",
    "            # this was not printing coz of leakyReLU wasnt correctly called(was only nn.ReLU)\n",
    "            #print(f'Hook called for {name}: mean={mean}, var={var}, neg_ratio={neg_ratio}')\n",
    "        return hook   \n",
    "    \n",
    "    # Register hooks for GELU/ReLU layers depends (or whatever activation you're using)\n",
    "    for name, layer in net.named_modules():\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            layer.register_forward_hook(get_activation_stats(name))\n",
    "            #print(f'Registered hook for layer: {name}') #-----debugging print worked\n",
    "            \n",
    "    def capture_gradient(name):\n",
    "        def hook(module, grad_input, grad_output):\n",
    "            gradients.append((name, grad_output[0].detach()))\n",
    "        return hook\n",
    "    \n",
    "    iteration = 0\n",
    "    while iteration < iteration_count:\n",
    "        epoch_loss = 0\n",
    "        batches = 0\n",
    "        \n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            lrs.append(param_group['lr'])\n",
    "        \n",
    "        print('learning rate', lrs[-1])\n",
    "        wandb.log({'learning rate': lrs[-1]})\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if iteration >= iteration_count:\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            norm = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "            epoch_loss += loss.item()\n",
    "            batches += 1\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "            \n",
    "                #Compute values  for wandb logging\n",
    "                current_step = batch_idx * len(data)\n",
    "                total_steps = len(train_loader.dataset)\n",
    "                percentage_complete = 100. * batch_idx / len(train_loader)\n",
    "                loss_value = loss.item()\n",
    "        \n",
    "                # Log the values with wandb\n",
    "                wandb.log({\n",
    "                    'epoch': epoch,\n",
    "                    'current_step': current_step,\n",
    "                    'total_steps': total_steps,\n",
    "                    'percentage_complete': percentage_complete,\n",
    "                    'loss': loss_value\n",
    "                })\n",
    "                \n",
    "            # Average the epoch loss over batches\n",
    "            #fixing the different length of lr and losses items (10 vs 630) by averaging loss per epoch\n",
    "        activations_list.append({k: {stat: v[stat][-batches:] for stat in v} for k, v in activations_dict.items()})\n",
    "        losses.append(epoch_loss / batches)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        with torch.no_grad():\n",
    "            #ud.append([(lr*p.grad.std()/ p.data.std().log10().item()) for p in net.parameters()])\n",
    "            epsilon = 1e-8\n",
    "            ud.append([(current_lr * p.grad.std() / (p.data.std() + epsilon).log10().item()) for p in net.parameters() if p.grad is not None])\n",
    "    \n",
    "       \n",
    "        \n",
    "\n",
    "    plot_activation_stats(activations_list)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses, lrs)\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.xlabel('Loss')\n",
    "    plt.title('Learning Rate vs. Loss')\n",
    "    plt.show()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Convert BytesIO to PIL Image\n",
    "    image1 = Image.open(buf)\n",
    "    wandb.log({\"Learning Rate vs. Loss\": wandb.Image(image1)})\n",
    "\n",
    "    return {\n",
    "        \"net\": net,\n",
    "        \"losses\": losses,\n",
    "        \"activations\": activations_list\n",
    "    }\n",
    "\n",
    "def plot_activation_stats(activations_list):\n",
    "    if not activations_list:\n",
    "        print(\"No activation data to plot.\")\n",
    "        wandb.log({'message':'No activation data to plot'})\n",
    "        return\n",
    "\n",
    "    for layer_name in activations_list[0].keys():\n",
    "        means = [epoch[layer_name]['mean'] for epoch in activations_list]\n",
    "        vars = [epoch[layer_name]['var'] for epoch in activations_list]\n",
    "        neg_ratios = [epoch[layer_name]['neg_ratio'] for epoch in activations_list]\n",
    "\n",
    "        # logging this stat only on wandb \n",
    "        print(f'Plotting data for layer: {layer_name}')\n",
    "        wandb.log({'Plotting data for layer':layer_name}) # logging this stat only on wandb \n",
    "        print(f'Means: {means}')\n",
    "        wandb.log({'Means': means})\n",
    "        print(f'Variances: {vars}')\n",
    "        wandb.log({'Variances': vars})\n",
    "        print(f'Negative Ratios: {neg_ratios}')\n",
    "        wandb.log({'Negative Ratios':neg_ratios})\n",
    "\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.plot(means)\n",
    "        plt.title(f'{layer_name} - Mean Activation')\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Mean')\n",
    "        \n",
    "        plt.subplot(132)\n",
    "        plt.plot(vars)\n",
    "        plt.title(f'{layer_name} - Activation Variance')\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Variance')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.plot(neg_ratios)\n",
    "        plt.title(f'{layer_name} - Negative Activation Ratio')\n",
    "        plt.xlabel('Batch')\n",
    "        plt.ylabel('Ratio')\n",
    "         \n",
    "        plt.tight_layout()\n",
    "        \n",
    "\n",
    "        # Save the plot to a buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "\n",
    "        # Convert BytesIO to PIL Image\n",
    "        image = Image.open(buf)\n",
    "\n",
    "        # Log the plot to Weights and Biases\n",
    "        wandb.log({f'{layer_name} activations': wandb.Image(image)})\n",
    "        plt.show() # this should be after wandb log \n",
    "\n",
    "        plt.close()  # Close the figure to free up memory\n",
    "        buf.close()\n",
    "\n",
    "\n",
    "training_result = training_model()\n",
    "model = training_result[\"net\"]\n",
    "\n",
    "plot_activation_stats(activations_list)\n",
    "\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cb6b7c0-750f-422b-b8bf-cb2cb5644438",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(net\u001b[38;5;241m.\u001b[39mparameters()): \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m----> 6\u001b[0m         plt\u001b[38;5;241m.\u001b[39mplot([ud[j][i]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mud\u001b[49m))])\n\u001b[1;32m      7\u001b[0m         legends\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparam \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ud)], [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# those ratios should be ~1e-3, indicated on the plot with black         \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ud' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize histogram of gradient\n",
    "plt.figure(figsize=(20,4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(net.parameters()): \n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i].cpu().numpy() for j in range(len(ud))])\n",
    "        legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # those ratios should be ~1e-3, indicated on the plot with black         \n",
    "plt.legend(legends);\n",
    "plt.title('update to data raio distribation, LR setting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83c118-b852-4020-b4e8-78b61f18ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "legends = []\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(list(net.parameters()))))\n",
    "for i, (p, color) in enumerate(zip(net.parameters(), colors)):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i].cpu().numpy() for j in range(len(ud))], color=color)\n",
    "        legends.append(f'param {i}')\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k')\n",
    "plt.legend(legends)\n",
    "plt.title('Update-to-Data Ratio Distribution, LR Setting')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Update-to-Data Ratio')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb615f27-9600-4edd-93cc-04a4ad8c04d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbde52-9856-4fcf-b1b8-29fd2cfb051b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666c6e4-b0f8-4259-ab37-d834e8d9c8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace4eb8-1fce-40d1-a070-cd7eae65f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  \n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import umap.plot\n",
    "import plotly.graph_objs as go \n",
    "import plotly.io as pio \n",
    "pio.renderers.default ='iframe'\n",
    "\n",
    "import math,os,sys\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a91ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_dataset import MNISTDataset \n",
    "#load dataset \n",
    "data = pd.read_csv('../data/train.csv')\n",
    "data = data[:10]\n",
    "#temporarly trying to overfit with less data\n",
    "val_count =2\n",
    "\n",
    "#common transformation\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n",
    "\n",
    "#split the train to val and train\n",
    "dataset = MNISTDataset(data.iloc[:-val_count], default_transform)\n",
    "val_dataset = MNISTDataset(data.iloc[-val_count:], default_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup Dataloaders with pytorch dataloaders\n",
    "trainloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = 2,\n",
    "    shuffle =True,\n",
    "    #pin_memory = True, # for faster data transfer speed btn CPU and GPU, but will consume more system memory\n",
    "    num_workers = 2,\n",
    "    #prefetch_factor = 100,#to specify how many batches should be prefetched(loaded into memory[increased memory usage tho]) asynchronously in advance.\n",
    "\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91ca38-4641-43fc-a054-9af4e0e45918",
   "metadata": {},
   "source": [
    "# visualizing Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a76d4-b95e-49ca-a943-7d9dd320ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title =''):\n",
    "    num_images = len(images)\n",
    "    fig,axes = plt.subplots(1, num_images,figsize=(9,3))\n",
    "    for i in range(num_images):\n",
    "        img = np.squeeze(images[i])\n",
    "        axes[i].imshow(img,cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    fig.suptitle(title)\n",
    "    plt.show()\n",
    "    \n",
    "for batch_idx, (anchor_images, contrastive_images, distances, labels) in enumerate(trainloader):\n",
    "    #converting tensors to numpy, numpy is easy to muniplate and display with matplotlib\n",
    "    anchor_images = anchor_images.numpy()\n",
    "    contrastive_images = contrastive_images.numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    #display some imgages from batch\n",
    "    show_images(anchor_images[:4], title = 'Anchor Images')\n",
    "    show_images(contrastive_images[:4], title = '+/- Example Images')\n",
    "    #break after displaying from one batch for demostration \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68115e5-2935-48f5-a968-12f4fe679b70",
   "metadata": {},
   "source": [
    "# lets build Neural Network\n",
    "-  Define a neural network architecture with two convolution layers and two fully connected layers\n",
    "- Input to the network is an MNIST image and Output is a 64 dimensional representation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f725e1e-0423-47f8-b173-8ab9d05296eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network import Network, ContrastiveLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4f49e1-4752-453a-8522-4ee463060cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "\n",
    "device= \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device= \"mps\"\n",
    "\n",
    "#device= \"cpu\" #overide device for overfitting a very small data batch\n",
    "net = net.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3245ae-de36-4899-9c01-6cffd815ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom initialization function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='gelu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd5839-6797-405c-8f91-ea0e2a7ba585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_for_gelu(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Initialize Conv2d weights with a scaled normal distribution\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # Initialize Linear weights with a scaled normal distribution\n",
    "        in_features = m.weight.size(1)\n",
    "        std = 1. / math.sqrt(in_features)\n",
    "        m.weight.data.normal_(0, std)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9b640-9d71-4e38-9d1d-8d090eb81628",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_count=10\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr *10 \n",
    "max_steps = epoch_count\n",
    "warm_up_steps = max_steps * 0.2\n",
    "\n",
    "def get_lr(iter):\n",
    "    #linear warm up for warm up steps\n",
    "    if iter < warm_up_steps:\n",
    "        return max_lr * (iter+1) / warm_up_steps\n",
    "    #if iter > lr_decay_iters, return min learning rate\n",
    "    if iter > max_steps:\n",
    "        return min_lr\n",
    "    #in between ,use cosine decay down to min learning rate\n",
    "    decay_ratio = (iter - warm_up_steps) / (max_steps - warm_up_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) #coffe starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e0e90e-1c06-4bcd-83f5-fcb7ad1570d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lre = torch.linspace(-3,0,100)\n",
    "# lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8b27f-d130-4697-a2cf-a1efe0de0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(net.parameters(), lr = 0.001)\n",
    "#optimizer = torch.optim.AdamW(net.parameters())\n",
    "loss_function = ContrastiveLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1074f81-4ec1-4d6f-90db-51f9425197f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_dir ='checkpoints/'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd524c75-d2b2-432f-a699-4612bfa7c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_checkpoint(net, epoch, checkpoint_dir='checkpoints'):\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "#     checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt')\n",
    "#     torch.save(net.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0421d17c-d595-4994-98dc-ca6b6b6c2ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lre = torch.linspace(-3,0,1000)\n",
    "# lrs = 10**lre\n",
    "#print(lrs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6d592-6e15-4163-bb9e-a7a4a56b9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "\n",
    "def Train_model(epoch_count=20):\n",
    "    net = Network()\n",
    "    net.apply(init_weights_for_gelu)\n",
    "    net = net.to(device)\n",
    "    lrs = []\n",
    "    losses = []\n",
    "    #--------\n",
    "    \n",
    "    \n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations.append((name, output.detach()))\n",
    "        return hook\n",
    "\n",
    "    # Register hooks for ReLU layers\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            layer.register_forward_hook(get_activation(layer.__class__.__name__))\n",
    "    #------------\n",
    "    for epoch in range(epoch_count):\n",
    "        epoch_loss = 0\n",
    "        batches = 0\n",
    "        print('epoch-',epoch) \n",
    "        for param_group in optimizer.param_groups:\n",
    "            lrs.append(param_group['lr'])\n",
    "        \n",
    "        print('learning rate',lrs[-1])\n",
    "\n",
    "        for anchor, contrastive, distance, label in tqdm(trainloader):\n",
    "            \n",
    "            # Ensure data is in the correct shape\n",
    "            assert anchor.shape[1] == 1, f\"Expected anchor channels to be 1, but got {anchor.shape[1]}\"\n",
    "            assert contrastive.shape[1] == 1, f\"Expected contrastive channels to be 1, but got {contrastive.shape[1]}\"\n",
    "\n",
    "            batches +=1\n",
    "            optimizer.zero_grad()\n",
    "            anchor_out = anchor.to(device, dtype=torch.float32)\n",
    "            contrastive_out = contrastive.to(device, dtype=torch.float32)\n",
    "            distance = distance.to(torch.float32).to(device)\n",
    "\n",
    "            anchor_out = net(anchor_out)\n",
    "            contrastive_out = net(contrastive_out)\n",
    "            \n",
    "            loss = loss_function(anchor_out, contrastive_out, distance)\n",
    "            epoch_loss += loss\n",
    "            loss.backward()\n",
    "            norm = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        losses.append(epoch_loss.cpu().detach().numpy()/ batches)\n",
    "        scheduler.step()\n",
    "        print('epoch_loss', losses[-1])\n",
    "\n",
    "        #save checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch{epoch}.pt')\n",
    "        torch.save(net.state_dict(), checkpoint_path)\n",
    "        #-------------------\n",
    "        # Visualize activations\n",
    "        # plt.figure(figsize=(20, 4))\n",
    "        # legends = []\n",
    "        # for name, t in activations:\n",
    "        #     print(f'{name}: mean: {t.mean():+.2f}, std: {t.std():.2f}, saturated: {(t.abs() > 0.97).float().mean() * 100:.2f}%')\n",
    "        #     hy, hx = torch.histogram(t, density=True)\n",
    "        #     plt.plot(hx[:-1].detach().cpu(), hy.detach().cpu())\n",
    "        #     legends.append(name)\n",
    "        # plt.legend(legends)\n",
    "        # plt.title('Activation Distribution')\n",
    "        # plt.show()\n",
    "        # activations.clear()  # Clear activations for the next epoch\n",
    "        #------------------------------\n",
    "        \n",
    "    # print(f\"lrs = {lrs} #######\")\n",
    "    # print(f\"losses = {losses} #####\")\n",
    "    plt.plot(lrs, losses)\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Rate vs. Loss')\n",
    "    plt.show()\n",
    "\n",
    "    return{\n",
    "        \"net\": net,\n",
    "        \"losses\":losses\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e7b1e-a73d-42db-b408-ff65c7342710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = True\n",
    "checkpoint_dir = 'checkpoints'\n",
    "\n",
    "\n",
    "training_result = Train_model()\n",
    "model = training_result[\"net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a4c38f-af81-4236-9b43-06dc8360948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize histogram of activations\n",
    "plt.figure(figsize=(20,4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,layer in enumerate(net): #not excluding the last layer since there is no softmax\n",
    "    if isinstance(layer, ReLU):\n",
    "        t = layer.out\n",
    "        print('layer %d (%10s): mean: %+.2f, std: %.2f, saturated: %.2f%%' %(i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].detach(), hy.detach())\n",
    "        legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08ad25-2856-4e31-863a-e4e9236fbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'activations' is a list of (name, tensor) tuples\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for name, t in activations:\n",
    "    # Calculate metrics\n",
    "    zero_percentage = (t == 0).float().mean() * 100\n",
    "    mean_value = t.mean().item()\n",
    "    std_value = t.std().item()\n",
    "    sparsity = (t != 0).float().mean() * 100\n",
    "    variance = t.var().item()\n",
    "\n",
    "    #print(f'{name}: mean: {mean_value:+.2f}, std: {std_value:.2f}, zeros: {zero_percentage:.2f}%, sparsity: {sparsity:.2f}%, variance: {variance:.2f}')\n",
    "    \n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach().cpu(), hy.detach().cpu())\n",
    "    legends.append(name)\n",
    "\n",
    "plt.legend(legends[:3])\n",
    "plt.title('Activation Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed3fde-5a90-40fc-997e-a3fa139ae9d2",
   "metadata": {},
   "source": [
    "# load from backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2782299-3a0c-457d-b580-c484b9cf82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_model_from_checkpoint():\n",
    "#     checkpoint = torch.load('checkpoints/model_epoch_99.pt')\n",
    "\n",
    "#     net = Network()\n",
    "#     net.load_state_dict(checkpoint)\n",
    "#     net.eval()\n",
    "\n",
    "#     return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e31e5-e4b4-4397-8243-abbe17562edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def save_checkpoint(net, epoch, checkpoint_dir='checkpoints'):\n",
    "#     os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "#     checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt')\n",
    "#     torch.save(net.state_dict(), checkpoint_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d4586-17dc-4c93-a313-c0ed577cca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "checkpoint_dir = 'checkpoints'\n",
    "\n",
    "if train:\n",
    "    training_result = Train_model()\n",
    "    model = training_result[\"net\"]\n",
    "else:\n",
    "    model = load_latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce5e0f-b759-491b-9ca3-0a19f9b82d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "plt.plot(training_result[losses])\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e488d8a3-d4a8-4c02-80b8-f9e188f3600f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

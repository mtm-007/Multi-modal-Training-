{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f56a95-1bc2-4a9b-9b1a-e0c795c8318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a32ce-e040-45a2-bcb6-bbf9d51d8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm  \n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import math\n",
    "import io\n",
    "import umap.plot\n",
    "import plotly.graph_objs as go \n",
    "import plotly.io as pio \n",
    "pio.renderers.default ='iframe'\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import random\n",
    "from PIL import Image  # Add this import statement\n",
    "\n",
    "%matplotlib inline\n",
    "import math,os,sys\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9924b34-e73e-4c66-a0d0-5e66ae0f4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Network import Network,Network_t ,ContrastiveLoss_with_margin, CustomVGG\n",
    "from utils import init_weights, init_weights_for_gelu,initialize_weights_mod,plot_activation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c85516-ad55-4113-b80b-6ed804b4febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.data.core import DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6f712-e2b7-4f24-8f41-7f36fd8698e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device= \"mps\"\n",
    "\n",
    "#device= \"cpu\" #overide device for overfitting a very small data batch\n",
    "#net = net.to(device)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb98d65-d571-467f-a1e5-1fc31059a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_cnn_dataset import TransformedDataset \n",
    "from mnist_dataset import MNISTDataset \n",
    "#load dataset \n",
    "data = pd.read_csv('../data/train.csv')\n",
    "#data_shorted = data[:10000]\n",
    "data_clp = data[:1000].reset_index(drop=True) # the index has to be readjusted otherwise it follows the whole data index... problematic\n",
    "val_count = 200\n",
    "#temporarly trying to overfit with less data\n",
    "\n",
    "\n",
    "#common transformation\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f1724-d3c9-420e-aa01-35aa3f606733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(data.iloc[:-val_count], default_transform)\n",
    "val_dataset = MNISTDataset(data.iloc[-val_count:], default_transform)\n",
    "\n",
    "\n",
    "# Define batch size and other DataLoader parameters\n",
    "batch_size = 128 * 4\n",
    "num_workers = 2\n",
    "\n",
    "# Create DataLoaders\n",
    "dls = DataLoaders.from_dsets(train_dataset, val_dataset, bs=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6032c4b-414e-412d-914d-82c8ccba79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import Learner\n",
    "from fastai.callback.all import Callback\n",
    "from fastai.learner import CancelStepException\n",
    "\n",
    "class ContrastiveLossCallback(Callback):\n",
    "    def __init__(self, loss_func):\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def after_pred(self):\n",
    "        embeddings1 = self.learn.pred[0]  # Assuming embeddings from the model\n",
    "        embeddings2 = self.learn.pred[1]  # Assuming embeddings from the model\n",
    "        distance = self.learn.y  # Assuming distance labels (0 or 1)\n",
    "\n",
    "        loss = self.loss_func(embeddings1, embeddings2, distance)\n",
    "        self.learn.loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f2d10-ac17-47d9-b9fd-0b7cc5a2f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = TransformedDataset(data_clp, default_transform)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(transformed_data))\n",
    "val_size = len(transformed_data) - train_size\n",
    "train_dataset, val_dataset = random_split(transformed_data, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ae3d6-5c22-4035-9f75-7d5053f85c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dls = DataLoaders(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd67a28-5170-48d9-8d12-5baea8cad78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(row):\n",
    "    # Assuming pixel values are in columns named 'pixel0', 'pixel1', etc.\n",
    "    img = [row[f'pixel{i}'] for i in range(784)]  # Adjust 784 if your image size is different\n",
    "    return torch.tensor(img).float().view(1,28, 28)  # Adjust 28, 28 to match your image dimensions\n",
    "\n",
    "def get_y(row):\n",
    "    return row['label']\n",
    "\n",
    "\n",
    "dblock = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_x=get_x,\n",
    "    get_y=get_y,\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    batch_tfms=default_transform\n",
    ")\n",
    "\n",
    "# Create the DataLoaders\n",
    "#dls = dblock.dataloaders(data_clp, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6417b9f-5533-4169-a812-c3fd7dc961fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.init(project=\"Contrastive_learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111619c-c4d1-447a-9b78-1728472dbe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomVGG()  # Initialize your custom model\n",
    "loss_func = ContrastiveLoss_with_margin()  # Initialize your contrastive loss function\n",
    "\n",
    "learner = Learner(dls, model, loss_func=loss_func, cbs=ContrastiveLossCallback(loss_func))\n",
    "learner.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c58b3-4545-4b60-b8c6-9a567a9e777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21246446-2ba9-4ec4-bc2e-91d34b3b9ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(pretrained=False):\n",
    "    return CustomVGG()\n",
    "model = CustomVGG()\n",
    "num_classes=10\n",
    "dls.c = num_classes\n",
    "#learner = vision_learner(dls=dls, arch= CustomVGG, metrics=error_rate)\n",
    "# Create the learner directly\n",
    "learner = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=error_rate,cbs=WandbCallback())\n",
    "learner.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7793b45-6e67-420f-a84c-612a16b47c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(10, lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d8c2e8-4dbd-44d8-9b6c-74dedb182206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learner.fine_tune(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85294339-ec86-4272-b7f1-507ac8f0e979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
